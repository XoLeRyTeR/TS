{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO7DtHBuUzCfUeYCuX9OZOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XoLeRyTeR/TS/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y_H7uQ2jnE4D"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "def find_face(video):\n",
        "  video_path = video\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "      gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "      if len(faces) > 0:\n",
        "          (x, y, w, h) = faces[0]\n",
        "          face = frame[y:y+h, x:x+w]\n",
        "          cap.release()\n",
        "          return face"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel, ViTFeatureExtractor\n",
        "import torch\n",
        "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "def img2vec(img):\n",
        "  #image = cv2.imread(img)\n",
        "  image=img\n",
        "  inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  feature_vector = outputs.last_hidden_state[:,0].detach().numpy()\n",
        "  return feature_vector"
      ],
      "metadata": {
        "id": "YmJvBFWQ1Kxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49737a8-1881-43f1-8b81-9c94249c63e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "smlaDgY3CWCV",
        "outputId": "bba052c4-a41d-4c52-cda4-f16e6ba2c520"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'face.jpg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "def extract_wav(video_path):\n",
        "  video = VideoFileClip(video_path)\n",
        "  audio = video.audio\n",
        "  audio.write_audiofile(\"audio.wav\", codec='pcm_s16le')\n",
        "  return \"audio.wav\""
      ],
      "metadata": {
        "id": "WPk0cW--eq-e"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j2AI-6x5qJh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "def wav2vec(audio_path):\n",
        "  y, sr = librosa.load(audio_path, sr=None)\n",
        "  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "  return np.array(np.mean(mfccs.T, axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKOEqmV9dkt-",
        "outputId": "6985eb71-15ac-4ef7-8136-e9e40b725272"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "[-552.75134    167.51488     10.217794     8.829773    34.56931\n",
            "   16.159134     3.0027163    2.5715618   -8.696403   -13.079629\n",
            "   -5.283581     2.0560398    1.7127628]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, LayerNormalization,Input\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "import numpy as np\n",
        "\n",
        "def video2frames(video_path):\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    step = total_frames // 30\n",
        "    frames = []\n",
        "\n",
        "    for i in range(30):\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    video.release()\n",
        "    #print(np.array(frames).shape)\n",
        "    return np.array(frames)\n",
        "def video2vec(frames):\n",
        "    features = np.array([img2vec(frame)[0] for frame in frames])\n",
        "    features = np.expand_dims(features, axis=0)\n",
        "    #print(features.shape[1:])\n",
        "\n",
        "    input_tensor = Input(shape=features.shape[1:])\n",
        "    attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(input_tensor, input_tensor)\n",
        "    normalized_output = LayerNormalization()(attention_output)\n",
        "    output_tensor = Dense(128, activation='relu')(normalized_output)\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    vector_out = model.predict(features)\n",
        "    return vector_out\n"
      ],
      "metadata": {
        "id": "Sa0K0MiYgSm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf67e8c8-f836-47d3-dcde-b7cd63165c01"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 720, 1280, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "[[[0.30882    0.37696663 0.         ... 0.9362     0.70999146 0.        ]\n",
            "  [0.30894    0.37673122 0.         ... 0.9363478  0.7097751  0.        ]\n",
            "  [0.3088261  0.37661493 0.         ... 0.936355   0.70965123 0.        ]\n",
            "  ...\n",
            "  [0.30891997 0.37656695 0.         ... 0.9364232  0.7096943  0.        ]\n",
            "  [0.30874133 0.3765542  0.         ... 0.9364667  0.7097348  0.        ]\n",
            "  [0.30879182 0.37659892 0.         ... 0.93645066 0.7096665  0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path='/content/videofolder/video.mp4'"
      ],
      "metadata": {
        "id": "jUxwuAIiZK42"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = {'wav2vec':wav2vec(extract_wav(video_path)),'img2vec':img2vec(find_face(video_path)),'video2vec': video2vec(video2frames(video_path))}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "OlQpdFqjb4ii",
        "outputId": "b93e541d-9994-43ae-f641-79a578925025"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "MoviePy error: the file /content/videofolder/video.mp4 could not be found!\nPlease check that you entered the correct path.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-5b2451ec3f89>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'wav2vec'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwav2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'img2vec'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mimg2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'video2vec'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvideo2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo2frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-3cb66419c56b>\u001b[0m in \u001b[0;36mextract_wav\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_wav\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_audiofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pcm_s16le'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Make a reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mpix_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"rgba\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhas_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"rgb24\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         self.reader = FFMPEG_VideoReader(filename, pix_fmt=pix_fmt,\n\u001b[0m\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0m\u001b[1;32m     36\u001b[0m                                    fps_source)\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"No such file or directory\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[0m\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                       \"path.\")%filename)\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file /content/videofolder/video.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9bXQ9hBDcd1s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}