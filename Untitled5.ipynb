{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM5YIwYUuHdkcI6ZhX+W/PP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XoLeRyTeR/TS/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "y_H7uQ2jnE4D"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "def find_face(video):\n",
        "  video_path = video\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  while cap.isOpened():\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "      gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "      faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "      if len(faces) > 0:\n",
        "          (x, y, w, h) = faces[0]\n",
        "          face = frame[y:y+h, x:x+w]\n",
        "          cap.release()\n",
        "          return face"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTModel, ViTFeatureExtractor\n",
        "import torch\n",
        "model = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "def img2vec(img):\n",
        "  #image = cv2.imread(img)\n",
        "  image=img\n",
        "  inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "  feature_vector = outputs.last_hidden_state[:,0].detach().numpy()\n",
        "  return feature_vector"
      ],
      "metadata": {
        "id": "YmJvBFWQ1Kxm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f49737a8-1881-43f1-8b81-9c94249c63e6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "def extract_wav(video_path):\n",
        "  video = VideoFileClip(video_path)\n",
        "  audio = video.audio\n",
        "  audio.write_audiofile(\"audio.wav\", codec='pcm_s16le')\n",
        "  return \"audio.wav\""
      ],
      "metadata": {
        "id": "WPk0cW--eq-e"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "def wav2vec(audio_path):\n",
        "  y, sr = librosa.load(audio_path, sr=None)\n",
        "  mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "  return np.array(np.mean(mfccs.T, axis=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKOEqmV9dkt-",
        "outputId": "6985eb71-15ac-4ef7-8136-e9e40b725272"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Writing audio in audio.wav\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "[-552.75134    167.51488     10.217794     8.829773    34.56931\n",
            "   16.159134     3.0027163    2.5715618   -8.696403   -13.079629\n",
            "   -5.283581     2.0560398    1.7127628]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, LayerNormalization,Input\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "import numpy as np\n",
        "\n",
        "def video2frames(video_path):\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    frames = []\n",
        "    total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    step = total_frames // 30\n",
        "    frames = []\n",
        "\n",
        "    for i in range(30):\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES, i * step)\n",
        "        ret, frame = video.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        frames.append(frame)\n",
        "\n",
        "    video.release()\n",
        "    #print(np.array(frames).shape)\n",
        "    return np.array(frames)\n",
        "def video2vec(frames):\n",
        "    features = np.array([img2vec(frame)[0] for frame in frames])\n",
        "    features = np.expand_dims(features, axis=0)\n",
        "    #print(features.shape[1:])\n",
        "\n",
        "    input_tensor = Input(shape=features.shape[1:])\n",
        "    attention_output = MultiHeadAttention(num_heads=8, key_dim=64)(input_tensor, input_tensor)\n",
        "    normalized_output = LayerNormalization()(attention_output)\n",
        "    output_tensor = Dense(128, activation='relu')(normalized_output)\n",
        "    model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    vector_out = model.predict(features)\n",
        "    return vector_out\n"
      ],
      "metadata": {
        "id": "Sa0K0MiYgSm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf67e8c8-f836-47d3-dcde-b7cd63165c01"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 720, 1280, 3)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step\n",
            "[[[0.30882    0.37696663 0.         ... 0.9362     0.70999146 0.        ]\n",
            "  [0.30894    0.37673122 0.         ... 0.9363478  0.7097751  0.        ]\n",
            "  [0.3088261  0.37661493 0.         ... 0.936355   0.70965123 0.        ]\n",
            "  ...\n",
            "  [0.30891997 0.37656695 0.         ... 0.9364232  0.7096943  0.        ]\n",
            "  [0.30874133 0.3765542  0.         ... 0.9364667  0.7097348  0.        ]\n",
            "  [0.30879182 0.37659892 0.         ... 0.93645066 0.7096665  0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path='/content/videofolder/video.mp4'"
      ],
      "metadata": {
        "id": "jUxwuAIiZK42"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data = {'wav2vec':wav2vec(extract_wav(video_path)),'img2vec':img2vec(find_face(video_path)),'video2vec': video2vec(video2frames(video_path))}"
      ],
      "metadata": {
        "id": "OlQpdFqjb4ii"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}